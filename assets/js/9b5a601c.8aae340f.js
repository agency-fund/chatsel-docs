"use strict";(self.webpackChunkchatsel_docs=self.webpackChunkchatsel_docs||[]).push([[2690],{1854:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"t2-behind-the-build/Chatbot Design/p3-chatbot-evaluation","title":"LLM Evals","description":"This page is under development...","source":"@site/docs/t2-behind-the-build/Chatbot Design/p3-chatbot-evaluation.md","sourceDirName":"t2-behind-the-build/Chatbot Design","slug":"/t2-behind-the-build/Chatbot Design/p3-chatbot-evaluation","permalink":"/chatsel-docs/docs/t2-behind-the-build/Chatbot Design/p3-chatbot-evaluation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/t2-behind-the-build/Chatbot Design/p3-chatbot-evaluation.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Knowledge Base","permalink":"/chatsel-docs/docs/t2-behind-the-build/Chatbot Design/p2-knowledge-base"},"next":{"title":"Minimal Viable Product (MVP)","permalink":"/chatsel-docs/docs/t2-behind-the-build/Chatbot Design/p4-no-code-demo"}}');var s=a(4848),i=a(8453);const o={sidebar_position:3},r="LLM Evals",l={},c=[{value:"What is &quot;eval&quot;?",id:"what-is-eval",level:2},{value:"A note on eval from a non-technical perspective",id:"a-note-on-eval-from-a-non-technical-perspective",level:3},{value:"What is our eval strategy for ChatSEL?",id:"what-is-our-eval-strategy-for-chatsel",level:2}];function d(e){const t={a:"a",admonition:"admonition",blockquote:"blockquote",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.header,{children:(0,s.jsx)(t.h1,{id:"llm-evals",children:"LLM Evals"})}),"\n",(0,s.jsx)(t.admonition,{type:"warning",children:(0,s.jsx)(t.p,{children:"This page is under development..."})}),"\n",(0,s.jsx)(t.h2,{id:"what-is-eval",children:'What is "eval"?'}),"\n",(0,s.jsx)(t.p,{children:'"Eval" stands for evaluation. Unlike "evaluation" in social impact projects, which typically refers to impact evaluation, "eval" in the context of GenAI usually refers to the systematic assessment of the performance of large language models (LLMs) and their associated applications.'}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.p,{children:"Evaluation metrics for LLM can be broadly classified into traditional and nontraditional metrics."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Traditional evaluation metrics rely on the arrangement and order of words and phrases in the text and are used in combination where a reference text (ground truth) exists to compare the predictions against."}),"\n",(0,s.jsx)(t.li,{children:"Nontraditional metrics make use of semantic structure and capabilities of language models for evaluating generated text."}),"\n"]}),"\n"]}),"\n",(0,s.jsx)("img",{src:a(6125).A}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsxs)(t.em,{children:["Reference: Please see this post for a comprehensive review of ",(0,s.jsx)(t.a,{href:"https://explodinggradients.com/all-about-evaluating-large-language-models",children:"LLM evals"}),"."]})}),"\n",(0,s.jsx)(t.h3,{id:"a-note-on-eval-from-a-non-technical-perspective",children:"A note on eval from a non-technical perspective"}),"\n",(0,s.jsx)(t.p,{children:"Imagine you\u2019re teaching a student. After a lesson, you\u2019d give them a test to see how well they\u2019ve understood the material \u2014 whether they can solve problems, answer questions correctly, and apply what they\u2019ve learned."}),"\n",(0,s.jsx)(t.p,{children:"An LLM eval is like that test, but for an AI language model instead of a student. It checks how well the AI performs at specific tasks, like answering questions accurately, summarizing information, or following instructions. Just as teachers use different tests for math, history, or writing, AI evaluations use different methods to see how good the model is at different skills."}),"\n",(0,s.jsx)(t.p,{children:"The goal is to figure out what the AI does well, where it makes mistakes, and how it can be improved \u2014 just like helping a student become a better learner."}),"\n",(0,s.jsx)(t.h2,{id:"what-is-our-eval-strategy-for-chatsel",children:"What is our eval strategy for ChatSEL?"}),"\n",(0,s.jsx)(t.admonition,{type:"warning",children:(0,s.jsx)(t.p,{children:"This section is under development..."})}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.a,{href:"https://docs.ragas.io/en/stable/concepts/metrics/overview/",children:"RAGAS metrics"})}),"\n",(0,s.jsx)("img",{src:a(4404).A})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},6125:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/llm-evals-875d91e903f756e069af5015714a0d6d.jpg"},4404:(e,t,a)=>{a.d(t,{A:()=>n});const n=a.p+"assets/images/ragas-metrics-9dd5320e9e0c8130ffb72301db36bab6.jpg"},8453:(e,t,a)=>{a.d(t,{R:()=>o,x:()=>r});var n=a(6540);const s={},i=n.createContext(s);function o(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),n.createElement(i.Provider,{value:t},e.children)}}}]);